# ğŸ“„ Cache Parsers - Parseurs de DonnÃ©es

## ğŸ“‹ Vue d'ensemble

Les parseurs du cache SAM sont responsables de la lecture, validation et transformation des donnÃ©es externes en modÃ¨les internes utilisables. Ils supportent multiple formats de donnÃ©es (GeoJSON, CSV, JSON) et sources (OSM, opÃ©rateurs, fichiers personnalisÃ©s).

## ğŸ—ï¸ Architecture

```
parsers/
â”œâ”€â”€ toll_booth_parser.py           # Parseur de pÃ©ages OSM
â”œâ”€â”€ motorway_link_parser.py        # Parseur de liaisons autoroutiÃ¨res
â”œâ”€â”€ motorway_segments_parser.py    # Parseur de segments autoroutiers
â”œâ”€â”€ multi_source_parser.py         # Parseur multi-sources
â””â”€â”€ __init__.py
```

## ğŸ¯ Objectifs des parseurs

### **FlexibilitÃ© des formats**
- ğŸ“Š **Multi-formats** : GeoJSON, CSV, JSON, XML support
- ğŸ”„ **Transformation** : Conversion vers modÃ¨les internes uniformes
- âœ… **Validation** : ContrÃ´les de cohÃ©rence et d'intÃ©gritÃ©
- ğŸŒ **Encoding** : Support UTF-8, encodages locaux

### **Robustesse**
- ğŸ›¡ï¸ **Gestion d'erreurs** : Traitement gracieux des donnÃ©es malformÃ©es
- ğŸ“Š **Statistiques** : MÃ©triques de parsing et validation
- ğŸ”„ **RÃ©cupÃ©ration** : StratÃ©gies de fallback et retry
- ğŸ“ **Logging** : TraÃ§abilitÃ© des opÃ©rations

## ğŸ§© Composants dÃ©taillÃ©s

### **1. TollBoothParser** - Parseur de PÃ©ages

Parseur spÃ©cialisÃ© pour les stations de pÃ©ages depuis les donnÃ©es OSM.

#### **FonctionnalitÃ©s**
- ğŸ—ºï¸ **Format OSM** : Lecture des donnÃ©es OpenStreetMap (GeoJSON)
- ğŸ·ï¸ **Enrichissement** : Ajout de mÃ©tadonnÃ©es et classification
- ğŸ¯ **Filtrage** : SÃ©lection par critÃ¨res (opÃ©rateur, autoroute, type)
- ğŸ“Š **Validation** : ContrÃ´les de cohÃ©rence gÃ©ographique

#### **Utilisation**
```python
from src.cache.parsers.toll_booth_parser import TollBoothParser

# Initialisation
parser = TollBoothParser(
    data_dir="./data",
    enable_validation=True,
    enable_enrichment=True
)

# Parsing des pÃ©ages depuis GeoJSON
toll_booths = parser.parse_toll_booths_from_geojson(
    file_path="./data/osm/toll_stations.geojson"
)

print(f"âœ… {len(toll_booths)} pÃ©ages parsÃ©s")

# Statistiques de parsing
stats = parser.get_parsing_statistics()
print(f"ğŸ“Š Statistiques:")
print(f"   - PÃ©ages valides: {stats['valid_tolls']}")
print(f"   - PÃ©ages rejetÃ©s: {stats['rejected_tolls']}")
print(f"   - PÃ©ages ouverts: {stats['open_tolls']}")
print(f"   - PÃ©ages fermÃ©s: {stats['closed_tolls']}")
```

#### **Structure GeoJSON attendue**
```json
{
  "type": "FeatureCollection",
  "features": [
    {
      "type": "Feature",
      "properties": {
        "highway": "toll_booth",
        "barrier": "toll_booth",
        "name": "PÃ©age de Fontaine LariviÃ¨re",
        "operator": "ASF",
        "operator:ref": "FL001",
        "ref": "A 6",
        "toll": "yes",
        "toll:type": "open"
      },
      "geometry": {
        "type": "Point",
        "coordinates": [4.8567, 46.7892]
      }
    }
  ]
}
```

#### **Enrichissement automatique**
```python
def enrich_toll_booth_data(self, toll_data):
    """Enrichit les donnÃ©es de pÃ©age avec des informations dÃ©duites."""
    
    enriched_data = toll_data.copy()
    
    # DÃ©tection du type de pÃ©age
    if "toll:type" in toll_data.get("properties", {}):
        toll_type = toll_data["properties"]["toll:type"]
        enriched_data["toll_type"] = "O" if toll_type == "open" else "F"
    else:
        # DÃ©duction basÃ©e sur l'opÃ©rateur et le nom
        if self.is_likely_open_toll(toll_data):
            enriched_data["toll_type"] = "O"
        else:
            enriched_data["toll_type"] = "F"
    
    # Normalisation du nom d'opÃ©rateur
    operator = toll_data.get("properties", {}).get("operator", "")
    enriched_data["operator"] = self.normalize_operator_name(operator)
    
    # Extraction de la rÃ©fÃ©rence d'autoroute
    highway_ref = toll_data.get("properties", {}).get("ref", "")
    enriched_data["highway_ref"] = self.normalize_highway_ref(highway_ref)
    
    # Ajout de mÃ©tadonnÃ©es gÃ©ographiques
    coords = toll_data["geometry"]["coordinates"]
    enriched_data["geographic_info"] = {
        "department": self.get_department_from_coordinates(coords),
        "region": self.get_region_from_coordinates(coords)
    }
    
    return enriched_data
```

---

### **2. MotorwayLinkParser** - Parseur de Liaisons AutoroutiÃ¨res

Parseur pour les points d'entrÃ©e et sortie des autoroutes.

#### **ResponsabilitÃ©s**
- ğŸ›£ï¸ **Points de liaison** : EntrÃ©es et sorties autoroutiÃ¨res
- ğŸ” **Classification** : DÃ©termination du type (entrÃ©e/sortie/indÃ©terminÃ©)
- ğŸ“ **GÃ©olocalisation** : Validation des coordonnÃ©es
- ğŸ·ï¸ **MÃ©tadonnÃ©es** : Enrichissement avec informations contextuelles

#### **Utilisation**
```python
from src.cache.parsers.motorway_link_parser import MotorwayLinkParser

# Initialisation
parser = MotorwayLinkParser(
    data_dir="./data",
    classification_enabled=True
)

# Parsing des liens autoroutiers
parsing_result = parser.parse_motorway_links(
    file_path="./data/osm/motorway_links.geojson"
)

entries = parsing_result['entries']
exits = parsing_result['exits'] 
indeterminates = parsing_result['indeterminates']

print(f"âœ… Parsing terminÃ©:")
print(f"   - EntrÃ©es: {len(entries)}")
print(f"   - Sorties: {len(exits)}")
print(f"   - IndÃ©terminÃ©s: {len(indeterminates)}")
```

#### **Algorithme de classification**
```python
def classify_motorway_link(self, link_data):
    """Classifie un lien autoroutier en entrÃ©e/sortie."""
    
    properties = link_data.get("properties", {})
    
    # Classification basÃ©e sur les tags OSM
    if "highway" in properties:
        highway_type = properties["highway"]
        
        if highway_type == "motorway_link":
            # Analyse des tags additionnels
            if "destination" in properties:
                return LinkType.EXIT  # Sortie vers destination
            elif "source" in properties:
                return LinkType.ENTRY  # EntrÃ©e depuis source
            elif "oneway" in properties:
                oneway = properties["oneway"]
                if oneway == "yes":
                    # Analyse de la direction basÃ©e sur la gÃ©omÃ©trie
                    return self.classify_by_geometry(link_data)
                else:
                    return LinkType.INDETERMINATE
    
    # Classification par analyse gÃ©omÃ©trique
    return self.classify_by_geometric_analysis(link_data)

def classify_by_geometric_analysis(self, link_data):
    """Classification basÃ©e sur l'analyse gÃ©omÃ©trique."""
    
    coordinates = link_data["geometry"]["coordinates"]
    
    if len(coordinates) < 2:
        return LinkType.INDETERMINATE
    
    # Calcul de l'orientation
    start_coord = coordinates[0]
    end_coord = coordinates[-1]
    
    bearing = self.calculate_bearing(start_coord, end_coord)
    
    # Analyse de la proximitÃ© aux autoroutes principales
    nearest_motorway = self.find_nearest_motorway(start_coord)
    
    if nearest_motorway:
        motorway_bearing = nearest_motorway.get_bearing()
        angle_diff = abs(bearing - motorway_bearing)
        
        # Logique de classification basÃ©e sur l'angle
        if angle_diff < 45:  # MÃªme direction gÃ©nÃ©rale
            return LinkType.ENTRY
        elif angle_diff > 135:  # Direction opposÃ©e
            return LinkType.EXIT
        else:
            return LinkType.INDETERMINATE
    
    return LinkType.INDETERMINATE
```

---

### **3. MotorwaySegmentsParser** - Parseur de Segments Autoroutiers

Parseur pour les segments complets d'autoroutes avec gÃ©omÃ©tries dÃ©taillÃ©es.

#### **FonctionnalitÃ©s**
- ğŸ›£ï¸ **Segments complets** : Parsing des tronÃ§ons autoroutiers
- ğŸ“ **Calculs gÃ©omÃ©triques** : Distances et orientations
- ğŸ·ï¸ **MÃ©tadonnÃ©es** : OpÃ©rateurs, rÃ©fÃ©rences, caractÃ©ristiques
- ğŸ”— **PrÃ©paration liaison** : DonnÃ©es pour le systÃ¨me de linking

#### **Utilisation**
```python
from src.cache.parsers.motorway_segments_parser import MotorwaySegmentsParser

# Initialisation
parser = MotorwaySegmentsParser(
    data_dir="./data",
    calculate_distances=True,
    simplify_geometry=True,
    tolerance_m=10.0
)

# Parsing des segments
segments = parser.parse_motorway_segments(
    file_path="./data/osm/motorway_segments.geojson"
)

print(f"âœ… {len(segments)} segments parsÃ©s")

# Analyse des segments
analysis = parser.analyze_segments(segments)
print(f"ğŸ“Š Analyse:")
print(f"   - Distance totale: {analysis['total_distance_km']:.1f} km")
print(f"   - OpÃ©rateurs: {len(analysis['operators'])}")
print(f"   - Autoroutes: {len(analysis['highways'])}")
```

#### **Traitement des gÃ©ometries**
```python
def process_segment_geometry(self, segment_data):
    """Traite la gÃ©omÃ©trie d'un segment autoroutier."""
    
    geometry = segment_data["geometry"]
    coordinates = geometry["coordinates"]
    
    processed_segment = {
        "original_geometry": geometry,
        "coordinates": coordinates,
        "geometry_info": {}
    }
    
    # Calcul de la distance
    if self.calculate_distances:
        distance_m = self.calculate_polyline_distance(coordinates)
        processed_segment["distance_km"] = distance_m / 1000.0
        processed_segment["geometry_info"]["distance_m"] = distance_m
    
    # Simplification de la gÃ©omÃ©trie si demandÃ©e
    if self.simplify_geometry and self.tolerance_m > 0:
        simplified_coords = self.simplify_coordinates(
            coordinates, self.tolerance_m
        )
        processed_segment["simplified_geometry"] = {
            "type": "LineString",
            "coordinates": simplified_coords
        }
        processed_segment["geometry_info"]["simplification_ratio"] = (
            len(simplified_coords) / len(coordinates)
        )
    
    # Calcul des bearings
    if len(coordinates) >= 2:
        start_bearing = self.calculate_bearing(coordinates[0], coordinates[1])
        end_bearing = self.calculate_bearing(coordinates[-2], coordinates[-1])
        
        processed_segment["geometry_info"]["start_bearing"] = start_bearing
        processed_segment["geometry_info"]["end_bearing"] = end_bearing
        processed_segment["geometry_info"]["bearing_change"] = (
            self.angle_difference(start_bearing, end_bearing)
        )
    
    # Calcul de la bounding box
    bbox = self.calculate_bounding_box(coordinates)
    processed_segment["bbox"] = bbox
    
    return processed_segment
```

---

### **4. MultiSourceParser** - Parseur Multi-Sources

Parseur unifiÃ© capable de traiter multiple sources et formats de donnÃ©es.

#### **FonctionnalitÃ©s**
- ğŸ”„ **Multi-formats** : GeoJSON, CSV, JSON, XML
- ğŸ“Š **AgrÃ©gation** : Fusion de donnÃ©es de sources multiples
- ğŸ” **DÃ©duplication** : Ã‰limination des doublons
- ğŸ¯ **Mapping** : Correspondance entre schÃ©mas diffÃ©rents

#### **Utilisation**
```python
from src.cache.parsers.multi_source_parser import MultiSourceParser

# Configuration des sources
sources_config = {
    "osm_tolls": {
        "file_path": "./data/osm/toll_stations.geojson",
        "format": "geojson",
        "parser": "toll_booth",
        "priority": 1
    },
    "operator_data": {
        "file_path": "./data/operators/toll_details.csv",
        "format": "csv",
        "parser": "toll_booth",
        "priority": 2
    },
    "custom_additions": {
        "file_path": "./data/custom/additional_tolls.json",
        "format": "json",
        "parser": "toll_booth",
        "priority": 3
    }
}

# Initialisation
parser = MultiSourceParser(
    sources_config=sources_config,
    enable_deduplication=True,
    enable_validation=True
)

# Parsing multi-sources
result = parser.parse_all_sources()

merged_data = result['merged_data']
source_stats = result['source_statistics']
conflicts = result['conflicts']

print(f"âœ… Parsing multi-sources terminÃ©:")
print(f"   - Ã‰lÃ©ments fusionnÃ©s: {len(merged_data)}")
print(f"   - Sources traitÃ©es: {len(source_stats)}")
print(f"   - Conflits dÃ©tectÃ©s: {len(conflicts)}")
```

#### **Algorithme de fusion**
```python
def merge_data_sources(self, sources_data):
    """Fusionne les donnÃ©es de sources multiples."""
    
    merged_data = {}
    conflicts = []
    
    # Tri par prioritÃ©
    sorted_sources = sorted(
        sources_data.items(),
        key=lambda x: x[1]['priority']
    )
    
    for source_name, source_data in sorted_sources:
        for item_id, item_data in source_data['data'].items():
            
            if item_id not in merged_data:
                # PremiÃ¨re occurrence
                merged_data[item_id] = {
                    'data': item_data,
                    'source': source_name,
                    'sources': [source_name]
                }
            else:
                # Conflit potentiel
                existing_data = merged_data[item_id]['data']
                
                if self.data_conflicts(existing_data, item_data):
                    conflicts.append({
                        'item_id': item_id,
                        'source1': merged_data[item_id]['source'],
                        'source2': source_name,
                        'data1': existing_data,
                        'data2': item_data
                    })
                    
                    # RÃ©solution du conflit basÃ©e sur la prioritÃ©
                    if source_data['priority'] < merged_data[item_id]['priority']:
                        merged_data[item_id]['data'] = item_data
                        merged_data[item_id]['source'] = source_name
                
                # Ajout de la source Ã  la liste
                merged_data[item_id]['sources'].append(source_name)
    
    return merged_data, conflicts

def data_conflicts(self, data1, data2):
    """DÃ©tecte les conflits entre deux ensembles de donnÃ©es."""
    
    # Champs critiques Ã  vÃ©rifier
    critical_fields = ['coordinates', 'operator', 'name', 'toll_type']
    
    for field in critical_fields:
        if field in data1 and field in data2:
            if data1[field] != data2[field]:
                # Conflit dÃ©tectÃ©
                return True
    
    return False
```

## ğŸ”§ Configuration et validation

### **Configuration globale des parseurs**
```python
# Configuration complÃ¨te des parseurs
PARSERS_CONFIG = {
    "toll_booth_parser": {
        "enable_validation": True,
        "enable_enrichment": True,
        "coordinate_precision": 6,
        "required_fields": ["name", "coordinates", "operator"],
        "default_toll_type": "F"
    },
    "motorway_link_parser": {
        "classification_enabled": True,
        "geometric_analysis": True,
        "bearing_tolerance": 45.0,
        "distance_threshold_m": 500.0
    },
    "motorway_segments_parser": {
        "calculate_distances": True,
        "simplify_geometry": True,
        "tolerance_m": 10.0,
        "min_segment_length_m": 100.0
    },
    "multi_source_parser": {
        "enable_deduplication": True,
        "enable_validation": True,
        "conflict_resolution": "priority",
        "max_sources": 10
    }
}
```

### **Validation des donnÃ©es parsÃ©es**
```python
def validate_parsed_data(parsed_data, data_type):
    """Valide les donnÃ©es parsÃ©es selon le type."""
    
    validation_rules = {
        "toll_booth": {
            "required_fields": ["osm_id", "coordinates", "operator"],
            "coordinate_range": {"lon": [-180, 180], "lat": [-90, 90]},
            "operator_whitelist": ["ASF", "APRR", "SANEF", "AREA", "COFIROUTE"]
        },
        "motorway_link": {
            "required_fields": ["osm_id", "coordinates", "link_type"],
            "link_types": ["entry", "exit", "indeterminate"],
            "min_coordinates": 2
        },
        "motorway_segment": {
            "required_fields": ["osm_id", "geometry", "highway_ref"],
            "min_geometry_points": 2,
            "max_distance_km": 1000.0
        }
    }
    
    rules = validation_rules.get(data_type, {})
    validation_result = {
        "is_valid": True,
        "errors": [],
        "warnings": [],
        "validated_count": 0,
        "total_count": len(parsed_data)
    }
    
    for item in parsed_data:
        item_errors = []
        
        # Validation des champs requis
        for field in rules.get("required_fields", []):
            if field not in item or item[field] is None:
                item_errors.append(f"Champ requis manquant: {field}")
        
        # Validation spÃ©cifique par type
        if data_type == "toll_booth":
            item_errors.extend(validate_toll_booth_specific(item, rules))
        elif data_type == "motorway_link":
            item_errors.extend(validate_motorway_link_specific(item, rules))
        elif data_type == "motorway_segment":
            item_errors.extend(validate_motorway_segment_specific(item, rules))
        
        if item_errors:
            validation_result["errors"].extend(item_errors)
            validation_result["is_valid"] = False
        else:
            validation_result["validated_count"] += 1
    
    return validation_result
```

## ğŸ“Š MÃ©triques et monitoring

### **Statistiques de parsing**
```python
def collect_parsing_statistics(parser_results):
    """Collecte les statistiques de parsing."""
    
    stats = {
        "parsing_time_ms": parser_results.get("parsing_time_ms", 0),
        "total_items": parser_results.get("total_items", 0),
        "valid_items": parser_results.get("valid_items", 0),
        "rejected_items": parser_results.get("rejected_items", 0),
        "enriched_items": parser_results.get("enriched_items", 0),
        "success_rate": 0.0,
        "enrichment_rate": 0.0,
        "processing_speed": 0.0  # items/second
    }
    
    if stats["total_items"] > 0:
        stats["success_rate"] = (stats["valid_items"] / stats["total_items"]) * 100
        stats["enrichment_rate"] = (stats["enriched_items"] / stats["total_items"]) * 100
    
    if stats["parsing_time_ms"] > 0:
        stats["processing_speed"] = stats["total_items"] / (stats["parsing_time_ms"] / 1000.0)
    
    return stats
```

### **Monitoring des erreurs**
```python
def monitor_parsing_errors(error_log):
    """Analyse et catÃ©gorise les erreurs de parsing."""
    
    error_categories = {
        "format_errors": [],
        "validation_errors": [],
        "geographic_errors": [],
        "encoding_errors": [],
        "unknown_errors": []
    }
    
    for error in error_log:
        error_type = classify_error(error)
        error_categories[error_type].append(error)
    
    return {
        "total_errors": len(error_log),
        "error_categories": error_categories,
        "most_common_error": find_most_common_error(error_log),
        "error_rate": calculate_error_rate(error_log)
    }
```