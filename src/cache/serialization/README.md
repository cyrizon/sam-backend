# üíæ Cache Serialization - S√©rialisation Haute Performance

## üìã Vue d'ensemble

Le syst√®me de s√©rialisation SAM optimise le stockage et la r√©cup√©ration des donn√©es g√©ospatiales complexes. Il utilise une approche hybride combinant s√©rialisation binaire pour les performances et JSON pour les m√©tadonn√©es lisibles.

## üèóÔ∏è Architecture

```
serialization/
‚îú‚îÄ‚îÄ cache_serializer.py           # S√©rialiseur principal
‚îú‚îÄ‚îÄ complete_links_serializer.py  # S√©rialiseur de liaisons autorouti√®res
‚îú‚îÄ‚îÄ cache_metadata.py             # Gestionnaire de m√©tadonn√©es
‚îî‚îÄ‚îÄ __init__.py
```

## üéØ Objectifs du syst√®me

### **Performance**
- ‚ö° **Chargement rapide** : S√©rialisation binaire (pickle) pour vitesse
- üóúÔ∏è **Compression** : R√©duction de l'espace disque (~60-70%)
- üß† **Optimisation m√©moire** : Structures de donn√©es compactes
- üì¶ **Cache persistant** : √âvite le retraitement √† chaque d√©marrage

### **Fiabilit√©**
- ‚úÖ **Validation d'int√©grit√©** : Checksums et version checking
- üîÑ **Gestion des versions** : Migration automatique des formats
- üìä **M√©tadonn√©es compl√®tes** : Timestamps, statistiques, validation
- üõ°Ô∏è **R√©cup√©ration d'erreurs** : Fallback vers rechargement des sources

## üß© Composants d√©taill√©s

### **1. CacheSerializer** - S√©rialiseur Principal

Le s√©rialiseur principal g√®re les donn√©es de p√©ages et les tarifs op√©rateurs.

#### **Fonctionnalit√©s**
- üíæ **Sauvegarde binaire** : Pickle optimis√© avec compression
- üìÑ **M√©tadonn√©es JSON** : Informations de version et validation
- üîç **D√©tection de changements** : Rechargement intelligent
- üìä **Statistiques** : M√©triques de performance

#### **Structure des fichiers**
```
cache_directory/
‚îú‚îÄ‚îÄ cache_data.bin           # Donn√©es binaires (p√©ages, tarifs)
‚îú‚îÄ‚îÄ metadata.json           # M√©tadonn√©es du cache
‚îú‚îÄ‚îÄ operators_data.bin      # Donn√©es sp√©cifiques aux op√©rateurs
‚îî‚îÄ‚îÄ cache_stats.json        # Statistiques de performance
```

#### **Utilisation**
```python
from src.cache.serialization.cache_serializer import CacheSerializer

# Initialisation
serializer = CacheSerializer("./osm_cache")

# Sauvegarde des donn√©es
toll_booths = [...]  # Liste des p√©ages
operators_data = {...}  # Donn√©es des op√©rateurs

success = serializer.save_cache_data(
    toll_booths=toll_booths,
    operators_data=operators_data,
    additional_metadata={"version": "2.1", "source": "OSM-2025-06"}
)

# Chargement des donn√©es
if serializer.cache_exists():
    toll_booths, operators_data = serializer.load_cache_data()
    print(f"‚úÖ {len(toll_booths)} p√©ages charg√©s depuis le cache")
else:
    print("‚ùå Cache non disponible, rechargement n√©cessaire")
```

#### **M√©tadonn√©es automatiques**
```python
# Le s√©rialiseur g√©n√®re automatiquement :
metadata = {
    "cache_version": "2.1.0",
    "creation_timestamp": "2025-06-30T10:30:45Z",
    "data_sources": {
        "osm_file": "toll_stations.geojson",
        "operators_file": "price_per_km.csv"
    },
    "statistics": {
        "toll_booths_count": 847,
        "operators_count": 15,
        "file_size_mb": 12.5,
        "compression_ratio": 0.35
    },
    "integrity": {
        "checksum": "a1b2c3d4e5f6...",
        "validated": True
    }
}
```

---

### **2. CompleteMotorwayLinksSerializer** - Liaisons Autorouti√®res

S√©rialiseur sp√©cialis√© pour les liaisons autorouti√®res compl√®tes avec optimisations g√©ospatiales.

#### **Fonctionnalit√©s sp√©cifiques**
- üõ£Ô∏è **G√©om√©tries optimis√©es** : Compression des coordonn√©es
- üîó **Index de liaisons** : Acc√®s rapide par ID ou r√©f√©rence
- üìä **Statistiques de linking** : M√©triques de construction
- üéØ **Association p√©ages** : Liens p√©ages ‚Üî segments

#### **Structure des fichiers**
```
cache_directory/
‚îú‚îÄ‚îÄ complete_motorway_links.bin    # Liaisons compl√®tes binaires
‚îú‚îÄ‚îÄ links_metadata.json           # M√©tadonn√©es des liaisons
‚îú‚îÄ‚îÄ linking_stats.json            # Statistiques de construction
‚îî‚îÄ‚îÄ orphaned_segments.json        # Segments non li√©s (debug)
```

#### **Utilisation**
```python
from src.cache.serialization.complete_links_serializer import CompleteMotorwayLinksSerializer

# Initialisation
links_serializer = CompleteMotorwayLinksSerializer("./osm_cache")

# Sauvegarde des liaisons compl√®tes
complete_links = [...]  # Liste des liaisons compl√®tes
linking_stats = {...}   # Statistiques de construction

success = links_serializer.save_complete_links(
    complete_links=complete_links,
    linking_statistics=linking_stats,
    toll_associations=toll_associations_data
)

# Chargement des liaisons
if links_serializer.links_cache_exists():
    links_data = links_serializer.load_complete_links()
    
    complete_links = links_data['complete_links']
    linking_stats = links_data['linking_statistics'] 
    toll_associations = links_data['toll_associations']
    
    print(f"‚úÖ {len(complete_links)} liaisons charg√©es")
    print(f"üìä {linking_stats['processing_time_ms']}ms de construction")
```

#### **Optimisations g√©ospatiales**
```python
# Compression des coordonn√©es (r√©duction ~40%)
def compress_geometry(coordinates: List[List[float]]) -> bytes:
    """Compresse les coordonn√©es g√©ographiques."""
    # Conversion en format binaire optimis√©
    # R√©duction de pr√©cision contr√¥l√©e (6 d√©cimales ‚Üí ~1m)
    
# Index spatial pour recherche rapide
spatial_index = {
    "highway_refs": {"A1": [link_ids], "A6": [link_ids]},
    "operators": {"APRR": [link_ids], "ASF": [link_ids]},
    "regions": {"lyon": [link_ids], "paris": [link_ids]}
}
```

---

### **3. CacheMetadata** - Gestionnaire de M√©tadonn√©es

Gestionnaire centralis√© des m√©tadonn√©es pour tous les composants de cache.

#### **Responsabilit√©s**
- üìã **Tracking des versions** : Gestion des formats de donn√©es
- üïê **Timestamps** : Suivi des cr√©ations et modifications
- üîç **Validation** : Contr√¥les d'int√©grit√© automatiques
- üìä **M√©triques** : Statistiques de performance et utilisation

#### **Structure des m√©tadonn√©es**
```python
@dataclass
class CacheMetadata:
    cache_version: str              # Version du format de cache
    creation_timestamp: str         # ISO timestamp de cr√©ation
    last_updated: str              # Derni√®re modification
    data_sources: Dict[str, str]   # Fichiers sources avec checksums
    cache_statistics: Dict         # Statistiques d√©taill√©es
    integrity_info: Dict           # Informations de validation
    format_version: str = "2.1.0"  # Version du format de s√©rialisation
```

#### **Utilisation**
```python
from src.cache.serialization.cache_metadata import CacheMetadata

# Cr√©ation de m√©tadonn√©es
metadata = CacheMetadata(
    cache_version="2.1.0",
    creation_timestamp=datetime.now().isoformat(),
    data_sources={
        "toll_stations.geojson": "checksum_abc123",
        "price_per_km.csv": "checksum_def456"
    },
    cache_statistics={
        "toll_booths_count": 847,
        "processing_time_ms": 1250,
        "memory_usage_mb": 15.2
    },
    integrity_info={
        "validated": True,
        "validation_timestamp": datetime.now().isoformat(),
        "validation_errors": []
    }
)

# Sauvegarde et chargement
metadata.save_to_file("./cache/metadata.json")
loaded_metadata = CacheMetadata.load_from_file("./cache/metadata.json")

# Validation
if loaded_metadata.is_valid():
    print("‚úÖ M√©tadonn√©es valides")
else:
    print("‚ùå M√©tadonn√©es corrompues")
```

#### **Contr√¥les d'int√©grit√©**
```python
def validate_cache_integrity(cache_dir: str) -> ValidationResult:
    """Valide l'int√©grit√© compl√®te du cache."""
    
    # V√©rification des fichiers requis
    required_files = [
        "cache_data.bin", "metadata.json", 
        "complete_motorway_links.bin", "links_metadata.json"
    ]
    
    # Validation des checksums
    for file_name in required_files:
        expected_checksum = metadata.data_sources.get(file_name)
        actual_checksum = calculate_file_checksum(file_path)
        
        if expected_checksum != actual_checksum:
            return ValidationResult(
                is_valid=False,
                error=f"Checksum mismatch for {file_name}"
            )
    
    # Validation des versions
    if metadata.format_version != CURRENT_FORMAT_VERSION:
        return ValidationResult(
            is_valid=False,
            error="Format version mismatch, migration required"
        )
    
    return ValidationResult(is_valid=True)
```

## üîß Configuration et optimisation

### **Configuration des s√©rialiseurs**
```python
# Configuration avanc√©e du s√©rialiseur principal
serializer = CacheSerializer(
    cache_dir="./osm_cache",
    compression_level=9,        # Compression maximale
    enable_checksums=True,      # Validation d'int√©grit√©
    backup_old_cache=True,      # Sauvegarde avant √©crasement
    max_cache_age_hours=24      # Revalidation apr√®s 24h
)

# Configuration du s√©rialiseur de liaisons
links_serializer = CompleteMotorwayLinksSerializer(
    cache_dir="./osm_cache",
    coordinate_precision=6,     # Pr√©cision des coordonn√©es (d√©cimales)
    enable_spatial_index=True,  # Index spatial pour recherches
    compress_geometry=True      # Compression des g√©om√©tries
)
```

### **Optimisations de performance**
```python
# Chargement parall√®le des composants
import asyncio

async def load_cache_parallel():
    """Charge les composants de cache en parall√®le."""
    tasks = [
        asyncio.create_task(serializer.load_cache_data_async()),
        asyncio.create_task(links_serializer.load_complete_links_async()),
        asyncio.create_task(metadata_manager.load_metadata_async())
    ]
    
    results = await asyncio.gather(*tasks)
    return results

# Utilisation
cache_data, links_data, metadata = asyncio.run(load_cache_parallel())
```

### **Gestion de la m√©moire**
```python
# Chargement avec gestion m√©moire optimis√©e
def load_cache_memory_efficient():
    """Charge le cache avec optimisation m√©moire."""
    
    # Chargement par chunks pour gros datasets
    chunk_size = 1000
    toll_booths = []
    
    for chunk in serializer.load_cache_data_chunked(chunk_size):
        toll_booths.extend(chunk)
        
        # Garbage collection p√©riodique
        if len(toll_booths) % 5000 == 0:
            gc.collect()
    
    return toll_booths

# Streaming pour tr√®s gros volumes
def stream_complete_links():
    """Stream les liaisons pour traitement en temps r√©el."""
    for link in links_serializer.stream_complete_links():
        yield link  # Traitement individuel sans charger tout en m√©moire
```

## üìä M√©triques de performance

### **Temps de s√©rialisation**
- **Sauvegarde p√©ages** : ~200-400ms (800+ p√©ages)
- **Sauvegarde liaisons** : ~500-800ms (liaisons compl√®tes)
- **M√©tadonn√©es** : ~10-20ms (JSON l√©ger)
- **Validation int√©grit√©** : ~100-200ms

### **Temps de d√©s√©rialisation**
- **Chargement p√©ages** : ~150-300ms
- **Chargement liaisons** : ~300-600ms  
- **Validation cache** : ~50-100ms
- **Index spatial** : ~100-200ms

### **Ratios de compression**
- **Donn√©es p√©ages** : ~65% de r√©duction
- **G√©om√©tries** : ~40% de r√©duction
- **M√©tadonn√©es** : ~30% de r√©duction
- **Cache global** : ~55% de r√©duction

### **Utilisation disque typique**
```
Cache complet (~20MB d√©compress√©) :
‚îú‚îÄ‚îÄ cache_data.bin              ~4.2MB (p√©ages + tarifs)
‚îú‚îÄ‚îÄ complete_motorway_links.bin ~6.8MB (liaisons + g√©om√©tries)  
‚îú‚îÄ‚îÄ metadata.json               ~0.1MB (m√©tadonn√©es)
‚îú‚îÄ‚îÄ links_metadata.json         ~0.2MB (m√©tadonn√©es liaisons)
‚îú‚îÄ‚îÄ linking_stats.json          ~0.1MB (statistiques)
‚îî‚îÄ‚îÄ orphaned_segments.json      ~0.3MB (debug)
Total sur disque : ~11.7MB (compression ~58%)
```

## üõ†Ô∏è Debug et maintenance

### **Outils de diagnostic**
```python
# Analyse du cache
def analyze_cache_health(cache_dir: str):
    """Analyse compl√®te de la sant√© du cache."""
    
    report = {
        "files_status": check_cache_files_integrity(cache_dir),
        "performance_metrics": get_cache_performance_metrics(cache_dir),
        "memory_usage": analyze_memory_usage(cache_dir),
        "recommendations": generate_optimization_recommendations(cache_dir)
    }
    
    return report

# Nettoyage du cache
def cleanup_cache(cache_dir: str, keep_backups: int = 3):
    """Nettoie les anciens fichiers de cache."""
    
    # Suppression des anciens backups
    cleanup_old_backups(cache_dir, keep_backups)
    
    # Suppression des fichiers temporaires
    cleanup_temp_files(cache_dir)
    
    # D√©fragmentation du cache si n√©cessaire
    if should_defragment_cache(cache_dir):
        defragment_cache_files(cache_dir)
```

### **Migration de versions**
```python
def migrate_cache_format(cache_dir: str, from_version: str, to_version: str):
    """Migre le cache d'une version √† une autre."""
    
    migration_strategies = {
        ("2.0.0", "2.1.0"): migrate_2_0_to_2_1,
        ("2.1.0", "2.2.0"): migrate_2_1_to_2_2
    }
    
    strategy = migration_strategies.get((from_version, to_version))
    if strategy:
        return strategy(cache_dir)
    else:
        raise ValueError(f"Migration {from_version} ‚Üí {to_version} not supported")
```
